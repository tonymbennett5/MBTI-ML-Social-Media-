{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\tonym\\anaconda3\\envs\\learn-env\\lib\\site-packages (from vaderSentiment) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tonym\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tonym\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->vaderSentiment) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonym\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->vaderSentiment) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tonym\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data frame \n",
    "df_clean = pd.read_csv(r\"C:\\Users\\tonym\\Documents\\Flatiron\\phase_5\\capstone\\MBTI-ML-Social-Media-\\data\\df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>Extrovert</th>\n",
       "      <th>Sensing</th>\n",
       "      <th>Thinking</th>\n",
       "      <th>Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>and  moments    sportscenter not top ten p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>finding the lack    these  very alarming  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one            course   which  say  know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear      enjoyed our conversation the other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>you  fired  that  another silly misconception...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  Extrovert  Sensing  Thinking  Judging  \\\n",
       "0  INFJ          0        0         0        1   \n",
       "1  ENTP          1        0         1        0   \n",
       "2  INTP          0        0         1        0   \n",
       "3  INTJ          0        0         1        1   \n",
       "4  ENTJ          1        0         1        1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  'I'm finding the lack of me in these posts ver...   \n",
       "2  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                       cleaned posts  \n",
       "0      and  moments    sportscenter not top ten p...  \n",
       "1     finding the lack    these  very alarming  s...  \n",
       "2   good one            course   which  say  know...  \n",
       "3   dear      enjoyed our conversation the other ...  \n",
       "4   you  fired  that  another silly misconception...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_sentiment_score = []\n",
    "\n",
    "for post in df_clean['cleaned posts']:\n",
    "    score = analyzer.polarity_scores(post)\n",
    "    nlp_sentiment_score.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating columns for sentiment scores. We can create a compound column and\n",
    "# then one for postive, negative and neutral scores\n",
    "df_clean[\"compound_sentiment\"] = [\n",
    "    score[\"compound\"] for score in nlp_sentiment_score\n",
    "]\n",
    "df_clean[\"positive_sent\"] = [score[\"pos\"] for score in nlp_sentiment_score]\n",
    "df_clean[\"negative_sent\"] = [score[\"neg\"] for score in nlp_sentiment_score]\n",
    "df_clean[\"neutral_sent\"] = [score[\"neu\"] for score in nlp_sentiment_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>Extrovert</th>\n",
       "      <th>Sensing</th>\n",
       "      <th>Thinking</th>\n",
       "      <th>Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>positive_sent</th>\n",
       "      <th>negative_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>and  moments    sportscenter not top ten p...</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>finding the lack    these  very alarming  s...</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one            course   which  say  know...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear      enjoyed our conversation the other ...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>you  fired  that  another silly misconception...</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  Extrovert  Sensing  Thinking  Judging  \\\n",
       "0  INFJ          0        0         0        1   \n",
       "1  ENTP          1        0         1        0   \n",
       "2  INTP          0        0         1        0   \n",
       "3  INTJ          0        0         1        1   \n",
       "4  ENTJ          1        0         1        1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  'I'm finding the lack of me in these posts ver...   \n",
       "2  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                       cleaned posts  compound_sentiment  \\\n",
       "0      and  moments    sportscenter not top ten p...              0.9826   \n",
       "1     finding the lack    these  very alarming  s...              0.9984   \n",
       "2   good one            course   which  say  know...              0.9985   \n",
       "3   dear      enjoyed our conversation the other ...              0.9985   \n",
       "4   you  fired  that  another silly misconception...              0.9917   \n",
       "\n",
       "   positive_sent  negative_sent  neutral_sent  \n",
       "0          0.178          0.113         0.709  \n",
       "1          0.206          0.106         0.688  \n",
       "2          0.224          0.098         0.678  \n",
       "3          0.176          0.067         0.757  \n",
       "4          0.207          0.163         0.630  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS TAGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., although generally computational applications use more fine-grained POS tags like 'noun-plural'. We can find the average of different parts of speech and add them to the clean data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tag_posts'] = df_clean['posts'].str.replace(re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "                                                      lambda match: match.group(2),\n",
    "                                                     )\n",
    "#replacing ||| with space \n",
    "df_clean['tag_posts'] = [post for post in df_clean[\"tag_posts\"].str.split(\"\\|\\|\\|\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging for each word \n",
    "df_clean[\"tagged_words\"] = df_clean[\"tag_posts\"].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of point of speech tags \n",
    "tag_set = set()\n",
    "\n",
    "for i, data in df_clean[\"tagged_words\"].iteritems():\n",
    "    for tup in data[0]:\n",
    "        tag_set.add(tup[1])\n",
    "\n",
    "tag_list = list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating average and standard deviation of point of speech tags for each user\n",
    "def pos_cat(x, tag):\n",
    "    return [len([y for y in line if y[1] == tag]) for line in x]\n",
    "\n",
    "\n",
    "for col in tag_list:\n",
    "    df_clean[\"POS_\" + col + \"_mean\"] = df_clean[\"tagged_words\"].apply(\n",
    "        lambda x: np.mean(pos_cat(x, col))\n",
    "    )\n",
    "    df_clean[\"POS_\" + col + \"_std\"] = df_clean[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(pos_cat(x, col))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags that I will be using are from stanford's nlp list. It's based on the penn Treebank POS tagset We can add them to our data frame. The definitions of the tagging set look like this: \n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "\n",
    "To read more about the Penn Tree Project:\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_dict = {\n",
    "    \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP\": [\"EX\", \"TO\"],\n",
    "    \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ\": [\"CC\", \"IN\"],\n",
    "    \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM\": [\"CD\"],\n",
    "    \"PRT\": [\"RP\"],\n",
    "    \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanford_tag(x, tag):\n",
    "    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "for col in tags_dict.keys():\n",
    "    df_clean[col + \"_avg\"] = df_clean[\"tagged_words\"].apply(\n",
    "        lambda x: np.median(stanford_tag(x, col))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>Extrovert</th>\n",
       "      <th>Sensing</th>\n",
       "      <th>Thinking</th>\n",
       "      <th>Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>cleaned posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>positive_sent</th>\n",
       "      <th>negative_sent</th>\n",
       "      <th>...</th>\n",
       "      <th>ADV_avg</th>\n",
       "      <th>CONJ_avg</th>\n",
       "      <th>DET_avg</th>\n",
       "      <th>NOUN_avg</th>\n",
       "      <th>NUM_avg</th>\n",
       "      <th>PRT_avg</th>\n",
       "      <th>PRON_avg</th>\n",
       "      <th>VERB_avg</th>\n",
       "      <th>._avg</th>\n",
       "      <th>X_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>and  moments    sportscenter not top ten p...</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.113</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>finding the lack    these  very alarming  s...</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.106</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one            course   which  say  know...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.098</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear      enjoyed our conversation the other ...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.067</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>you  fired  that  another silly misconception...</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.163</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  Extrovert  Sensing  Thinking  Judging  \\\n",
       "0  INFJ          0        0         0        1   \n",
       "1  ENTP          1        0         1        0   \n",
       "2  INTP          0        0         1        0   \n",
       "3  INTJ          0        0         1        1   \n",
       "4  ENTJ          1        0         1        1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  'I'm finding the lack of me in these posts ver...   \n",
       "2  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                       cleaned posts  compound_sentiment  \\\n",
       "0      and  moments    sportscenter not top ten p...              0.9826   \n",
       "1     finding the lack    these  very alarming  s...              0.9984   \n",
       "2   good one            course   which  say  know...              0.9985   \n",
       "3   dear      enjoyed our conversation the other ...              0.9985   \n",
       "4   you  fired  that  another silly misconception...              0.9917   \n",
       "\n",
       "   positive_sent  negative_sent  ...  ADV_avg CONJ_avg DET_avg  NOUN_avg  \\\n",
       "0          0.178          0.113  ...      1.0      2.0     1.0       4.0   \n",
       "1          0.206          0.106  ...      2.0      4.0     2.0       6.0   \n",
       "2          0.224          0.098  ...      2.0      1.0     1.0       4.0   \n",
       "3          0.176          0.067  ...      1.5      3.0     2.0       5.0   \n",
       "4          0.207          0.163  ...      2.0      2.0     2.0       5.0   \n",
       "\n",
       "   NUM_avg  PRT_avg  PRON_avg  VERB_avg  ._avg  X_avg  \n",
       "0      0.0      0.0       1.0       2.0    2.5    0.0  \n",
       "1      0.0      0.0       4.0       6.0    4.0    0.0  \n",
       "2      0.0      0.0       2.0       4.0    3.0    0.0  \n",
       "3      0.0      0.0       3.0       5.0    3.0    0.0  \n",
       "4      0.0      0.0       3.0       5.0    3.0    0.0  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated csv \n",
    "df_clean.to_csv(r\"C:\\Users\\tonym\\Documents\\Flatiron\\phase_5\\capstone\\MBTI-ML-Social-Media-\\data\\df_clean2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
